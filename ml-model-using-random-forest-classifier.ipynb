{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bc86acd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T20:21:17.797737Z",
     "iopub.status.busy": "2023-07-13T20:21:17.797304Z",
     "iopub.status.idle": "2023-07-13T20:21:47.970273Z",
     "shell.execute_reply": "2023-07-13T20:21:47.968882Z"
    },
    "papermill": {
     "duration": 30.181405,
     "end_time": "2023-07-13T20:21:47.974366",
     "exception": false,
     "start_time": "2023-07-13T20:21:17.792961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nptdms\r\n",
      "  Downloading npTDMS-1.7.0.tar.gz (176 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.4/176.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\r\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from nptdms) (1.23.5)\r\n",
      "Building wheels for collected packages: nptdms\r\n",
      "  Building wheel for nptdms (pyproject.toml) ... \u001b[?25l-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for nptdms: filename=npTDMS-1.7.0-py3-none-any.whl size=104710 sha256=d8249b4915d9aafb5a574b0bc80de4b660e4d9d6b4224be2848cb58b9b9d0576\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/24/61/d6/bdb3426f132dc4ae7ecb0606257c0cb108f332025008d29750\r\n",
      "Successfully built nptdms\r\n",
      "Installing collected packages: nptdms\r\n",
      "Successfully installed nptdms-1.7.0\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nptdms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93eedd16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T20:21:47.982272Z",
     "iopub.status.busy": "2023-07-13T20:21:47.981859Z",
     "iopub.status.idle": "2023-07-13T20:21:49.669408Z",
     "shell.execute_reply": "2023-07-13T20:21:49.668286Z"
    },
    "papermill": {
     "duration": 1.693984,
     "end_time": "2023-07-13T20:21:49.671531",
     "exception": false,
     "start_time": "2023-07-13T20:21:47.977547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nptdms import TdmsFile\n",
    "\n",
    "\n",
    "#for traning data Librares:\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "255e83c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-13T20:21:49.679222Z",
     "iopub.status.busy": "2023-07-13T20:21:49.678884Z",
     "iopub.status.idle": "2023-07-13T20:21:51.714389Z",
     "shell.execute_reply": "2023-07-13T20:21:51.712882Z"
    },
    "papermill": {
     "duration": 2.042272,
     "end_time": "2023-07-13T20:21:51.717086",
     "exception": false,
     "start_time": "2023-07-13T20:21:49.674814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "healthy_data frame1 (12455, 66)\n",
      "Unhealthy_data_frame2 (3667, 66)\n",
      "healthy_data_frame3 (3147, 66)\n",
      "unhealthy_data_frame4 (7584, 66)\n",
      "Printing Whole dataset shape \n",
      " (19269, 65)\n",
      "\n",
      " DONE... Pre processing completed successfully and Model is ready to be trained \n",
      " \n",
      "Traing my Model Started...\n",
      " \n",
      "extracting arguments \n",
      "\n",
      "........Reading Data from CSV files........\n",
      "......Building Training and Testing Datasets....\n",
      " \n",
      "....Using RandomForestClassifier Algorithm and Fitting the Model....\n",
      " \n",
      "Printing Classification Report \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      3885\n",
      "           1       1.00      1.00      1.00       933\n",
      "\n",
      "    accuracy                           1.00      4818\n",
      "   macro avg       1.00      1.00      1.00      4818\n",
      "weighted avg       1.00      1.00      1.00      4818\n",
      "\n",
      "Trying to get the absolute Value at 10, 50, 90 percentile \n",
      " \n",
      "AE-at-10th-percentile: 0.0\n",
      "AE-at-50th-percentile: 0.0\n",
      "AE-at-90th-percentile: 0.0\n",
      "Model is created at /kaggle/working/model.joblib\n"
     ]
    }
   ],
   "source": [
    "def read_tdms(file_path_key):\n",
    "  \n",
    "    tdms_file = TdmsFile.read(file_path_key)\n",
    "\n",
    "    data = {}\n",
    "    channels = []\n",
    "    for group in tdms_file.groups():\n",
    "        for channel in group.channels():\n",
    "            channel_name = channel.name\n",
    "            data[channel_name] = channel[:]\n",
    "            channels.append(channel_name)\n",
    "   \n",
    "    return data, channels\n",
    "\n",
    "def preprocessing():\n",
    "    file_path_key1 = '/kaggle/input/tdms-datasets/Healthy_Q110_R3_05_16_2023_12_52_47.tdms'\n",
    "    file_path_key2 = '/kaggle/input/tdms-datasets/Unhealthy_Q110_R5_05_16_2023_15_08_23.tdms'\n",
    "    file_path_key3 = '/kaggle/input/tdms-datasets/Healthy_Q110_R18_05_16_2023_13_26_57.tdms'\n",
    "    file_path_key4 = '/kaggle/input/tdms-datasets/Unhealthy_R4_05_24_2023_11_52_49.tdms'\n",
    "\n",
    "\n",
    "    healthy_data_df1, healthy_channels_df1    = read_tdms(file_path_key1)\n",
    "    unhealthy_data_df1, unhealthy_channels_df1 = read_tdms(file_path_key2)\n",
    "    healthy_data_df2, healthy_channels_df2    = read_tdms(file_path_key3)\n",
    "    unhealthy_data_df2, unhealthy_channels_df2 = read_tdms(file_path_key4)\n",
    "    \n",
    "    \n",
    "    df1=pd.DataFrame(healthy_data_df1)\n",
    "    df1['Status'] = [0] * len(df1)\n",
    "    print(\"healthy_data frame1\",df1.shape)\n",
    "    \n",
    "    df2=pd.DataFrame(unhealthy_data_df1)\n",
    "    df2['Status'] = [1] * len(df2)\n",
    "    print(\"Unhealthy_data_frame2\", df2.shape)\n",
    "    \n",
    "    df3=pd.DataFrame(healthy_data_df2)\n",
    "    df3['Status'] = [0] * len(df3)\n",
    "    print(\"healthy_data_frame3\",df3.shape)\n",
    "    \n",
    "    df4=pd.DataFrame(unhealthy_data_df2)\n",
    "    df4['Status'] = [1] * len(df4)\n",
    "    print(\"unhealthy_data_frame4\", df4.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    data = pd.concat([df1, df2, df3, df4], ignore_index=True)\n",
    "    \n",
    "    if 'Time' in data.columns:\n",
    "        df = data.drop('Time', axis=1)\n",
    "        \n",
    "        \n",
    "    # Check for missing values in the DataFrame\n",
    "    missing_values = df.isnull().sum()\n",
    "    # print(missing_values)\n",
    "\n",
    "    df.drop(['IW4-0623-0-CHAN-4', 'P-0201-0-CHAN-3', 'P-0201-0-CHAN-4'], axis=1, inplace=True)\n",
    "\n",
    "    df.dropna(axis=0, inplace=True)\n",
    "    print(\"Printing Whole dataset shape \\n\", df.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    desired_order = ['B7043_18A', 'B5395_18A', 'B5384_18A', 'B5408_18A', 'IW4-0626-0-CHAN-1', 'IW4-0626-0-CHAN-2', 'LV219701_20A', 'B7046_18A', 'B4524_18A', 'B7032_18A', 'B5401_18A', 'B7030_18A', 'IW4-0623-0-CHAN-1', 'A2149', 'B7650_19A', 'B7035_18A', 'B6178_18A', 'B5397_18A', 'B7031_18A', 'B4546_18A', 'IW4-0627-0-CHAN-1', 'IW4-0627-0-CHAN-2', 'A2146_18A', 'B7045_18A', 'LV4231', 'B7033_18A', 'B7056_18A', 'B7639_19A', 'B5412_18A', 'B7037_18A', 'B7062_19A', 'LV207519_18A', 'B5381_18A', 'B6182_18A', 'B5398_18A', 'B7645_19A', 'B7648_19A', 'A2148', 'B6192_18A', 'B7643_19A', 'B7051_18A', 'B7061_18A', 'B4526_18A', 'B7038_18A', 'B7057_18A', 'B4541_18A', 'B7036_18A', 'B7039_18A', 'B7048_18A', 'B7049_18A', 'B5411_18A', 'B5406_18A', 'B5404_18A', 'B4523_18A', 'IW4-0624-0-CHAN-3', 'IW4-0624-0-CHAN-4', 'B5410_18A', 'B7055_18A', 'B7054_18A', 'B6181_18A', 'B7029_18A', 'B4531_18A', 'B7044_18A', 'B5382_18A', 'Status']\n",
    "\n",
    "    df= df.reindex(columns=desired_order)\n",
    "\n",
    "    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop(\"Status\", axis=1), df[\"Status\"], test_size=0.25, random_state=42\n",
    "    )\n",
    "\n",
    "    trainX = pd.DataFrame(X_train)\n",
    "    trainX[\"Status\"] = y_train\n",
    "\n",
    "    testX = pd.DataFrame(X_test)\n",
    "    testX[\"Status\"] = y_test  \n",
    "    \n",
    "    \n",
    "    \n",
    "    trainX.to_csv(\"/kaggle/working/Bridge_train.csv\")\n",
    "    testX.to_csv(\"/kaggle/working/Bridge_test.csv\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n DONE... Pre processing completed successfully and Model is ready to be trained \\n \")\n",
    "    train_path  = \"/kaggle/working/Bridge_train.csv\"\n",
    "    test_path   = \"/kaggle/working/Bridge_test.csv\"\n",
    "\n",
    "    return train_path, test_path\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_path, test_path = preprocessing()\n",
    "    print(\"Traing my Model Started...\\n \")\n",
    "    print(\"extracting arguments \\n\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    # to simplify  we  use all sklearn RandomForest hyperparameters\n",
    "    parser.add_argument(\"--n-estimators\", type=int, default=10)\n",
    "    parser.add_argument(\"--min-samples-leaf\", type=int, default=3)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=\"/kaggle/working/\")\n",
    "    parser.add_argument(\"--train-path\", type=str, default=train_path)\n",
    "    parser.add_argument(\"--test-path\", type=str, default=test_path)\n",
    "    \n",
    "    \n",
    "    # Adding Feature and Target Columns:\n",
    "    features= 'B7043_18A B5395_18A B5384_18A B5408_18A IW4-0626-0-CHAN-1 IW4-0626-0-CHAN-2 LV219701_20A B7046_18A B4524_18A B7032_18A B5401_18A B7030_18A IW4-0623-0-CHAN-1 A2149 B7650_19A B7035_18A B6178_18A B5397_18A B7031_18A B4546_18A IW4-0627-0-CHAN-1 IW4-0627-0-CHAN-2 A2146_18A B7045_18A LV4231 B7033_18A B7056_18A B7639_19A B5412_18A B7037_18A B7062_19A LV207519_18A B5381_18A B6182_18A B5398_18A B7645_19A B7648_19A A2148 B6192_18A B7643_19A B7051_18A B7061_18A B4526_18A B7038_18A B7057_18A B4541_18A B7036_18A B7039_18A B7048_18A B7049_18A B5411_18A B5406_18A B5404_18A B4523_18A IW4-0624-0-CHAN-3 IW4-0624-0-CHAN-4 B5410_18A B7055_18A B7054_18A B6181_18A B7029_18A B4531_18A B7044_18A B5382_18A'\n",
    "    target ='Status'\n",
    "    \n",
    "    \n",
    "    parser.add_argument(\n",
    "        \"--features\", type=str , default=features\n",
    "    )  # in this script we ask user to explicitly name features\n",
    "    parser.add_argument(\n",
    "        \"--target\", type=str , default= target\n",
    "    )  # in this script we ask user to explicitly name the target\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"........Reading Data from CSV files........\")\n",
    "    \n",
    "    train_df = pd.read_csv(args.train_path)\n",
    "    test_df = pd.read_csv(args.test_path)\n",
    "\n",
    "    print(\"......Building Training and Testing Datasets....\\n \")\n",
    "    X_train = train_df[args.features.split()]\n",
    "    X_test = test_df[args.features.split()]\n",
    "    y_train = train_df[args.target]\n",
    "    y_test = test_df[args.target]\n",
    "\n",
    "    # train\n",
    "    print(\"....Using RandomForestClassifier Algorithm and Fitting the Model....\\n \")\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=args.n_estimators, min_samples_leaf=args.min_samples_leaf, n_jobs=-1\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Printing Classification Report \\n \",classification_report(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    # print abs error\n",
    "    print(\"Trying to get the absolute Value at 10, 50, 90 percentile \\n \")\n",
    "    abs_err = np.abs(model.predict(X_test) - y_test)\n",
    "    \n",
    "    \n",
    "    # print couple perf metrics\n",
    "    for q in [10, 50, 90]:\n",
    "        print(\"AE-at-\" + str(q) + \"th-percentile: \" + str(np.percentile(a=abs_err, q=q)))\n",
    "\n",
    "    # persist model\n",
    "    path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model, path)\n",
    "    print(\"Model is created at \" + path)\n",
    "    # print(\"min_samples_leaf is \", args.min_samples_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42270a9f",
   "metadata": {
    "papermill": {
     "duration": 0.003736,
     "end_time": "2023-07-13T20:21:51.725209",
     "exception": false,
     "start_time": "2023-07-13T20:21:51.721473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 43.494367,
   "end_time": "2023-07-13T20:21:52.653232",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-13T20:21:09.158865",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
